# Embeddings, ch. 5, NLP w/ PyTorch

1. Other than words, what other discrete types can be used with embeddings?

1. The authors refer to Term-Frequency-Inverse-Document-Frequency (TF-IDF) as
   an example of a count-based embedding method. How is a count-based embedding
   different from learning-based or prediction-based?

1. What is a *distributional representation* of words? (Firth, 1935)

1. What is the
   [curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality)?

1. How do embeddings break the curse?

1. How are one-hot vocabulary vectors relevant to word embeddings?

1. What does *self-supervised machine learning* mean?

1. What is the difference between *language modeling*, *CBOW*, and *skipgrams*?
   Which of these were part of the groundbreaking *word2vec* paper (2013)?
   (extra reading: https://towardsdatascience.com/the-three-main-branches-of-word-embeddings-7b90fa36dfb9)

1. Describe the typical file format for sharing embeddings.

1. Describe how word embeddings are used to perform the analogy task.

1. Why are cooccurences not always a good way to encode meaning? Can you give
   an example?

1. What are some examples of positive and negative cultural biases that can be
   baked in to embeddings?
