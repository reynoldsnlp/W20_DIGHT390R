# Quiz for Interlingual MT / UNL / Pivot language

[BLEU at your own risk](https://towardsdatascience.com/evaluating-text-output-in-nlp-bleu-at-your-own-risk-e8609665a213)

1. BLEU stands for...
   * ...Bitext Language Exactness evalUation
   * ...BiLingual Evaluation Understudy
   * ...Belikov, Landry, Emmitt, Udaltsov (names of team who created it)
   * ...BLoomenthal EvalUation metric

1. True / False: BLEU is the de facto translation evaluation metric.<br/><br/>

1. True / False: BLEU is the consensus best translation evaluation metric.<br/><br/>

1. The author lists two major strengths of the BLEU metric. Which of the following is NOT one of them?
   * It’s fast and easy to calculate, especially compared to having human translators rate model output.
   * It's the best general-purpose metric. It doesn't fit _every_ MT task but it fits _most_ MT tasks.
   * It’s ubiquitous. This makes it easy to compare your model to benchmarks on the same task.

1. True / False: BLEU was always intended to be a corpus-level (i.e., not sentence-level) measure.

1. The author lists 4 drawbacks of BLEU. Which of the following is NOT one of them?
   * It doesn’t directly consider sentence structure
   * It doesn’t handle morphologically rich languages well
   * It doesn’t consider meaning
   * It doesn’t map well to human judgements
   * It doesn't consider discursive/pragmatic elements.
